# 1 summary

https://sthalles.github.io/simple-self-supervised-learning/

### åè§£

- tripletä¸‰å…ƒç»„ï¼šä¸‰å…ƒç»„æ˜¯æŒ‡ä¸€ç»„ä¸‰ä¸ªè¾“å…¥æ ·æœ¬ï¼Œç”¨äºè®­ç»ƒæ¨¡å‹ä»¥å­¦ä¹ åµŒå…¥ã€‚å…·ä½“æ¥è¯´ï¼Œä¸‰å…ƒç»„ç”±**é”šæ ·æœ¬ã€æ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬**ç»„æˆã€‚anchoræ ·æœ¬æ˜¯å‚è€ƒè¾“å…¥ï¼Œæ­£æ ·æœ¬æ˜¯ä¸anchorå±äºåŒä¸€ç±»æˆ–ç±»åˆ«çš„æ ·æœ¬ï¼Œè€Œè´Ÿæ ·æœ¬æ˜¯ä¸anchorå±äºä¸åŒç±»æˆ–ç±»åˆ«çš„æ ·æœ¬ã€‚

  æ—¨åœ¨å­¦ä¹ ä¸€ä¸ªå‡½æ•°ï¼Œå°†æ¯ä¸ªinputæ˜ å°„ä¸ºembeddingå‘é‡ï¼Œ

  ä½¿é”š~æ­£ è·ç¦»æœ€å°ï¼Œé”š~è´Ÿè·ç¦»æœ€å¤§ï¼Œtriplet lossç”¨æ¥è®¡ç®—è¿™ä¸¤ç§æŸå¤±

  åœ¨è®­ç»ƒæœŸé—´ï¼Œæ¨¡å‹ä¼šå‘ˆç°ä¸€ç³»åˆ—ä¸‰å…ƒç»„ï¼Œå¹¶ä¸ºæ¯ä¸ªä¸‰å…ƒç»„è®¡ç®—ä¸‰å…ƒç»„æŸå¤±ã€‚ç›®æ ‡æ˜¯ä¼˜åŒ–æ¨¡å‹å‚æ•°ä»¥æœ€å°åŒ–æ•´ä¸ªè®­ç»ƒé›†çš„ä¸‰å…ƒç»„æŸå¤±ã€‚é€šè¿‡è¿™æ ·åšï¼Œè¯¥æ¨¡å‹å­¦ä¹ ç”ŸæˆåµŒå…¥ï¼Œ**è¿™äº›åµŒå…¥å¯¹äºç›¸ä¼¼çš„æ ·æœ¬é å¾—å¾ˆè¿‘ï¼Œè€Œå¯¹äºä¸åŒçš„æ ·æœ¬åˆ™ç›¸è·å¾ˆè¿œã€‚**

- latent spaceæ½œåœ¨ç©ºé—´ï¼šæ•è·åŸå§‹é«˜ç»´æ•°æ®çš„é‡è¦å’Œç›¸å…³ç‰¹å¾çš„**ä½ç»´ç©ºé—´ï¼Œ**æ˜¯è¾“å…¥æ•°æ®çš„**å‹ç¼©è¡¨ç¤º**ï¼Œå¯ç”¨äºå„ç§ä¸‹æ¸¸ä»»åŠ¡ã€‚åœ¨å¯¹æ¯”å­¦ä¹ ä¸­ï¼Œå®ƒæŒ‡æ•°æ®çš„å·²ç»è¢«å­¦ä¹ åˆ°çš„ç‰¹å¾ï¼Œå¯åº”ç”¨åœ¨è®¡ç®—å¯¹æ¯”æŸå¤±ä¸Šäº†

- ablation studyæ¶ˆèç ”ç©¶ï¼šç”¨äºé€šè¿‡ç§»é™¤æˆ–ä¿®æ”¹æ¨¡å‹çš„ç»„ä»¶æ¥ç ”ç©¶æ¨¡å‹æ€§èƒ½çš„æŠ€æœ¯ï¼Œä»¥äº†è§£æ¯ä¸ªç»„ä»¶å¯¹æ¨¡å‹æ•´ä½“æ€§èƒ½çš„è´¡çŒ®ã€‚æ¶ˆèç ”ç©¶è¦æ±‚æ¨¡å‹è¡¨ç°å‡ºâ€œä¼˜é›…çš„é€€åŒ–â€ï¼Œè¿™æ„å‘³ç€å³ä½¿æŸäº›ç»„ä»¶è¢«ç§»é™¤æˆ–é€€åŒ–ï¼Œæ¨¡å‹ä¹Ÿåº”è¯¥ç»§ç»­è¿è¡Œã€‚

- optimizerä¼˜åŒ–ç®—æ³•ï¼š**è°ƒæ•´NNçš„æƒé‡ä»¥minLoss**

  - ADAMï¼šè‡ªé€‚åº”lrä¼˜åŒ–
  - SGDï¼šç»å…¸ï¼Œå›ºå®šlrï¼Œä½†éœ€è¦æ›´å¤šçš„è¶…å‚è°ƒæ•´å’Œæ›´æ…¢çš„æ”¶æ•›

- schedulerè°ƒåº¦å™¨ï¼šæ˜¯ä¸€ç§åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­**æ›´æ–°optimizerå­¦ä¹ ç‡lr**çš„ç®—æ³•ã€‚

  - CosineAnnealingLRä½™å¼¦é€€ç«ï¼šä»¥éå¸¸å¤§çš„å­¦ä¹ ç‡å¼€å§‹ï¼Œç„¶ååœ¨ä¸‹æ¬¡å¢åŠ å­¦ä¹ ç‡ä¹‹å‰å°†å…¶å¿«é€Ÿé™ä½åˆ°æ¥è¿‘ 0 çš„å€¼ã€‚



### å¯¹æ¯”å­¦ä¹ ä»‹ç»

å¯¹æ¯”å­¦ä¹ çš„æ ¸å¿ƒæ˜¯å™ªå£°å¯¹æ¯”ä¼°è®¡ï¼ˆNCEï¼‰æŸå¤±ã€‚

![image-20230301144920398](https://tkmfpicgo.oss-cn-hangzhou.aliyuncs.com/img/image-20230301144920398.png) 

> - **x+**æ˜¯è¾“å…¥æ•°æ®xçš„åŸºå‡†ç‚¹ï¼Œx+ä¸xç›¸å…³ï¼Œä¸”ä¸¤è€…ï¼ˆx,x+ï¼‰ä¸ºä¸€ä¸ªæ­£æ ·æœ¬å¯¹ã€‚
>
>   é€šå¸¸x+æ˜¯xçš„å˜æ¢ç»“æœï¼Œå˜æ¢å¯èƒ½æœ‰å°ºå¯¸åˆ‡å‰²ã€æ—‹è½¬ã€ç§ç§æ•°æ®å¢å¼ºç­‰
>
>   :star:**å¼ºæ•°æ®å¢å¼º**å¯¹æ— ç›‘ç£å­¦ä¹ éå¸¸æœ‰ç”¨ï¼Œä½œè€…æ¨èéšæœºè£å‰ªã€æ°´å¹³ç¿»è½¬ã€è‰²å½©æŠ–åŠ¨ã€é«˜æ–¯æ¨¡ç³Š
>
> - è€Œ**x-**æ˜¯ä¸xä¸ç›¸è¿‘çš„æ ·æœ¬ï¼Œï¼ˆx,x-ï¼‰ç»„æˆä¸€ä¸ªè´Ÿæ ·æœ¬å¯¹ä¸”ä¸ç›¸å…³
>
> - **æ¯ä¸ªæ­£å¯¹éƒ½æœ‰kä¸ªè´Ÿå¯¹**
>
>   :star:è¯•éªŒç»“æœè¡¨æ˜**éœ€è¦å¤§é‡è´Ÿå¯¹**æ¥ä¿è¯æ•ˆæœ
>
> - NCE lossæ—¨åœ¨æ”¾å¤§æ­£å¯¹ä¸è´Ÿå¯¹çš„å·®åˆ«
>
>   - **simï¼ˆï¼‰**å‡½æ•°æ˜¯ç›¸ä¼¼æ€§ï¼ˆè·ç¦»ï¼‰åº¦é‡ï¼Œè´Ÿè´£æœ€å°åŒ–æ­£å¯¹ä¹‹é—´çš„è·ç¦»ï¼Œæœ€å¤§åŒ–æ­£å¯¹ä¸è´Ÿå¯¹çš„è·ç¦»
>
>     é€šå¸¸å®ƒçš„å½¢å¼æ˜¯**ä½™å¼¦ç›¸ä¼¼æ€§**æˆ–**ç‚¹ç§¯**
>
>   - **gï¼ˆï¼‰**å‡½æ•°æ˜¯ä¸€ä¸ªCNNencoderï¼ˆresnet50ï¼‰ï¼Œæœ€è¿‘çš„å¯¹æ¯”å­¦ä¹ æ¶æ„ä¼šç”¨å­ªç”Ÿç½‘ç»œå­¦ä¹ æ­£å¯¹ä¸è´Ÿå¯¹çš„embeddingï¼Œä¹‹åå°†å…¶è¿å»ç®—å¯¹æ¯”æŸå¤±

ç®€å•æ¥è¯´ï¼Œæˆ‘ä»¬å¯ä»¥å°†å¯¹æ¯”ä»»åŠ¡è§†ä¸ºç”¨è®¡ç®—è·ç¦»åœ¨ä¸€å †è´Ÿä¾‹ä¸­æ‰¾å‡ºæ­£ä¾‹ã€‚

### simCLR

è¯¥æ–¹æ³•åœ¨è‡ªç›‘ç£å’ŒåŠç›‘ç£å­¦ä¹ åŸºå‡†æµ‹è¯•ä¸­è¾¾åˆ°äº† SOTA

> state-of-the-artï¼ŒSOTA DNNæ˜¯å¯ç”¨äºä»»ä½•ç‰¹å®šä»»åŠ¡çš„**æœ€ä½³æ¨¡å‹**ã€‚å¦‚æœ DNN åœ¨æ€§èƒ½å‡†ç¡®åº¦ä¸Šå¾—åˆ†å¾ˆé«˜ï¼ˆå¤§çº¦ 90%-95%ï¼‰ï¼Œåˆ™å®ƒè¢«æ ‡è®°ä¸º SOTA æ¨¡å‹ã€‚

SimCLR ä½¿ç”¨å¯¹æ¯”å­¦ä¹ æ¥æœ€å¤§åŒ–åŒä¸€å›¾åƒçš„ 2 ä¸ªå¢å¼ºç‰ˆæœ¬ä¹‹é—´çš„ä¸€è‡´æ€§ã€‚

##### æ„å»º

1. ç»™å®šä¸€ä¸ªè¾“å…¥åŸå›¾ï¼Œåº”ç”¨ä¸¤ä¸ªä¸åŒçš„æ•°æ®å¢å¼ºæ‰‹æ®µåˆ›å»ºä¸¤ä¸ª**å¢å¼ºå‰¯æœ¬å›¾ç‰‡**

2. å°†æ‰€æœ‰å‰¯æœ¬å›¾ç‰‡è£…å…¥batch

3. :star2:å› ä¸ºè¦æœ€å¤§åŒ–è´Ÿå¯¹çš„æ•°é‡ï¼Œæ‰€ä»¥å°†ä¸€ä¸ªbatchå†…çš„ä¸€å¼ å›¾ä¸å…¶ä»–å›¾è¿›è¡Œç»„åˆæˆè´Ÿå¯¹ï¼ˆä¸èƒ½ä¸è‡ªèº«orå¦ä¸€ä¸ªå‰¯æœ¬ç»„åˆï¼‰

   è‹¥batchå¤§å°ä¸ºNï¼Œåˆ™æ¯å¼ å›¾éƒ½èƒ½ç»„æˆï¼ˆN-1ï¼‰ä¸ªè´Ÿå¯¹

4. ç”¨resnet-50ä½œä¸ºconvnetä¸»å¹²**fï¼ˆï¼‰**ï¼Œæ¥æ”¶å½¢çŠ¶ä¸º**(224,224,3)**çš„**å¢å¼ºå›¾åƒ**å¹¶è¾“å‡º 2048 ç»´embeddingå‘é‡**H**

5. å°†hè¾“å…¥åˆ°**æŠ•å½±å¤´gï¼ˆï¼‰**â€”â€”ç”±ä¸¤ä¸ªdense layersï¼ˆ2048ä¸ªå…ƒçš„å…¨è¿æ¥å±‚ï¼‰ç»„æˆçš„ä¸€ä¸ªMLPï¼Œéšå±‚æœ‰éçº¿æ€§æ¿€æ´»å‡½æ•°reluï¼Œ

   å¾—åˆ°**å‘é‡Z**â€”â€”æ½œåœ¨ç©ºé—´

   > model.add(Dense(32, input_dim=2048))
   >
   > MLPæ˜¯ä¸€ç§å…¨è¿æ¥çš„å‰é¦ˆäººå·¥ç¥ç»ç½‘ç»œï¼Œè¾“å…¥â€”â€”è‹¥å¹²éšå±‚â€”â€”è¾“å‡º

   <img src="https://tkmfpicgo.oss-cn-hangzhou.aliyuncs.com/img/image-20230301193707676.png" alt="image-20230301193707676" style="zoom:80%;" /> 

6. ç”¨å¯¹æ¯”æŸå¤±å‡½æ•°

   - å…ˆç”¨**ä½™å¼¦ç›¸ä¼¼åº¦**ï¼ˆ 2 ä¸ªéé›¶å‘é‡ä¹‹é—´å¤¹è§’çš„ä½™å¼¦å€¼ï¼‰æµ‹é‡æ­£å¯¹ä¸è´Ÿå¯¹é—´çš„å€¼
   - æœ‰äº†**ç›¸ä¼¼çŸ©é˜µ**ä¹‹åï¼Œæˆ‘ä»¬æ‰§è¡Œä¸€ä¸ª**softmax**æ¥å¾—åˆ°æ•´ä¸ªæ¨¡å‹çš„æ¦‚ç‡åˆ†å¸ƒ
   - ç›®çš„æ˜¯ä½¿softmaxåˆ†å¸ƒ

   

7. è®­ç»ƒç»“æŸåï¼Œä¸¢å¼ƒæŠ•å½±å¤´gï¼Œç›´æ¥ç”¨ä¸»å¹²å¾—åˆ°çš„HæŠ•å…¥ä¸‹æ¸¸ä»»åŠ¡



### æ¨¡å‹æ€§èƒ½è¯„ä¼°

##### sth

ä»…å‡†ç¡®æ€§Accuracyï¼ˆæ­£ç¡®æ•°/æ€»æ•°ï¼‰å¯èƒ½ä¸è¶³ä»¥è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚æ ¹æ®å…·ä½“ä»»åŠ¡ä»¥åŠè€ƒè™‘

1. å‡é˜³æ€§å’Œå‡é˜´æ€§ä¹‹é—´çš„å¹³è¡¡

   > é‡è¦çš„æ˜¯è¦è€ƒè™‘å…·ä½“ä»»åŠ¡ä»¥åŠæ¯ç§é”™è¯¯ç±»å‹çš„ç›¸å…³æˆæœ¬æˆ–é£é™©ã€‚
   >
   > åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå‡é˜³æ€§å¯èƒ½æ¯”å‡é˜´æ€§æˆæœ¬æ›´é«˜æˆ–é£é™©æ›´å¤§ï¼Œä¾‹å¦‚åœ¨åŒ»ç–—è¯Šæ–­ä»»åŠ¡ä¸­ï¼Œå‡é˜´æ€§ï¼ˆå®é™…æ‚£æœ‰ç–¾ç—…ä½†è¢«é”™è¯¯è¯Šæ–­ä¸ºå¥åº·çš„æ‚£è€…ï¼‰å¯èƒ½æ¯”å‡é˜³æ€§ï¼ˆå¥åº·æ‚£è€…è¢«è¯Šæ–­æ‚£æœ‰ç–¾ç—…ï¼‰æˆæœ¬æ›´é«˜æˆ–é£é™©æ›´å¤§ã€‚
   >
   > åœ¨è¯¯æŠ¥å’Œæ¼æŠ¥ä¹‹é—´å–å¾—å¹³è¡¡çš„ä¸€ç§æ–¹æ³•æ˜¯è°ƒæ•´æ¨¡å‹çš„å†³ç­–é˜ˆå€¼ã€‚ ï¼ˆä¸ºå‡é˜´å’Œå‡é˜³è°ƒæ•´æƒé‡ï¼‰

2. ç²¾åº¦Precisionï¼ˆçœŸé˜³/é¢„é˜³ï¼‰â€”â€”å¤šå°‘é”™æŠ“

3. å¬å›ç‡Recallï¼ˆçœŸé˜³/æ€»é˜³ï¼‰â€”â€”å¤šå°‘æ¼æŠ“

4.  F1 åˆ†æ•°â€”â€”ç²¾åº¦å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡æ•°*harmonic mean*ï¼Œè¶Šé«˜è¶Šå¥½

5. ROCreceiver operating characteristic

##### çº¿æ€§è¯„ä¼°åè®®

linear **evaluation** protocolç”¨äºè¯„ä¼°è®­ç»ƒå¥½çš„æ¨¡å‹ ç”Ÿæˆå‘é‡è¡¨å¾çš„æ°´å¹³

1. ä»è®­ç»ƒå¥½çš„ç¥ç»ç½‘ç»œä¸­æå–å­¦ä¹ åˆ°çš„è¡¨å¾ã€‚
2. ä½¿ç”¨æœ‰æ ‡è®°çš„ä¸‹æ¸¸æ•°æ®é›†ï¼Œåœ¨å­¦ä¹ çš„**è¡¨å¾ä¹‹ä¸Šè®­ç»ƒçº¿æ€§åˆ†ç±»å™¨**ã€‚
3. 
4. 



1. åŠ è½½trainedæ¨¡å‹
2. å°†é¢„è®­ç»ƒæ¨¡å‹çš„æœ€åä¸€å±‚æ›¿æ¢ä¸ºé€‚åˆæ–°ä»»åŠ¡çš„æ–°å±‚
3. å†»ç»“é™¤æœ€åä¸€å±‚ä»¥å¤–çš„æ‰€æœ‰å›¾å±‚ï¼ˆä¸æ”¹å˜æ¨¡å‹æƒé‡ï¼‰
4. ä½¿ç”¨æ–°æ•°æ®åœ¨æ–°ä»»åŠ¡ä¸Šè®­ç»ƒæ¨¡å‹ã€‚ç”±äºå¤§éƒ¨åˆ†å±‚éƒ½è¢«å†»ç»“ï¼Œå› æ­¤è®­ç»ƒè¿‡ç¨‹å°†æ¯”ä»å¤´å¼€å§‹è®­ç»ƒæ–°æ¨¡å‹å¿«å¾—å¤šã€‚
5. **fine-tune**â€”â€”è§£å†»éƒ¨åˆ†å±‚ï¼Œå†æ¬¡è®­ç»ƒ



è¿™ä¸ªæƒ³æ³•æ˜¯åœ¨ SimCLR ç¼–ç å™¨çš„å›ºå®šè¡¨å¾ä¸Šè®­ç»ƒçº¿æ€§åˆ†ç±»å™¨ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬è·å–è®­ç»ƒæ•°æ®ï¼Œå°†å…¶ä¼ é€’ç»™é¢„è®­ç»ƒçš„ SimCLR æ¨¡å‹ï¼Œå¹¶å­˜å‚¨è¾“å‡ºè¡¨ç¤ºã€‚æ³¨æ„ï¼Œæ­¤æ—¶æˆ‘ä»¬ä¸éœ€è¦æŠ•å½±å¤´Gäº†ã€‚

ç„¶åä½¿ç”¨è¿™äº›å›ºå®šè¡¨ç¤ºæ¥ä½¿ç”¨è®­ç»ƒæ ‡ç­¾ä½œä¸ºç›®æ ‡æ¥è®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥æµ‹é‡æµ‹è¯•ç²¾åº¦ï¼Œå¹¶å°†å…¶ç”¨ä½œç‰¹å¾è´¨é‡çš„åº¦é‡ã€‚

### simclrå…¶å®ƒå¾…è¯»

- åœ¨åŠç›‘ç£åŸºå‡†ä¸Šè¿›è¡Œæ— ç›‘ç£å¯¹æ¯”ç‰¹å¾å­¦ä¹ çš„ç»“æœï¼›

- å‘æŠ•å½±å¤´æ·»åŠ éçº¿æ€§å±‚çš„å®éªŒå’Œå¥½å¤„ï¼›

- ä½¿ç”¨å¤§batch sizeçš„å®éªŒå’Œå¥½å¤„ï¼›

- å¯¹æ¯”ç›®æ ‡è®­ç»ƒå¤§å‹æ¨¡å‹çš„ç»“æœï¼›

- ä½¿ç”¨å¤šç§æ›´å¼ºçš„æ•°æ®å¢å¼ºæ–¹æ³•è¿›è¡Œå¯¹æ¯”å­¦ä¹ çš„æ¶ˆèç ”ç©¶ï¼›

- å½’ä¸€åŒ–embeddingåœ¨è®­ç»ƒå¯¹æ¯”å­¦ä¹ æ¨¡å‹çš„å¥½å¤„ï¼›

  > å¯ç”¨äºè®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆåªä¸è§’åº¦æœ‰å…³ï¼Œå‘é‡å¤§å°æ— å…³ï¼‰

# 2  å®æ“

https://www.kaggle.com/code/aritrag/simclr

### 1æ•°æ®å¢å¼º+ç”Ÿæˆæ•°æ®é›†

> contrastive_learning_dataset.py

```python
# s

```





```python
#k	
class CustomDataset(Dataset):

    def __init__(self, list_images, transform=None):
        """
        Args:
            list_images (list): List of all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.list_images = list_images
        self.transform = transform

    def __len__(self):
        return len(self.list_images)

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
            
        img_name = self.list_images[idx]
        image = io.imread(img_name)
        if self.transform:
            image = self.transform(image)

        return image
    
# 
```



### 2æ•°æ®é›†å¯¼å…¥

> ä½äºrun.pyä¸­çš„mainå‡½æ•°

pytorchä¸­ä¸¤ç±»åŸå§‹å¯¼å…¥å‡½æ•°

- torch.utils.data.DataSet
- torch.utils.data.DataLoader

DataSetç±» å°è£…åŸå§‹æ•°æ®ï¼ŒDataLoaderç±» éå†åˆ›å»ºçš„DataSet

```python
# s
	#ğŸ‘‡æ•°æ®é›†ï¼ˆå‹ç¼©ï¼‰
data='./datasets' #self.root_folder
	#ContrastiveLearningDatasetä¸­å®šä¹‰
dataset = ContrastiveLearningDataset(data)
	
    #è¯»å–torchè‡ªå¸¦æ•°æ®é›†
def get_dataset(self, name, n_views):
        valid_datasets = {'cifar10': lambda: datasets.CIFAR10(self.root_folder, train=True, transform=ContrastiveLearningViewGenerator( self.get_simclr_pipeline_transform(32), n_views), download=True),
    #â­
train_dataset = dataset.get_dataset(dataset_name, n_views)

train_loader = torch.utils.data.DataLoader(
    train_dataset, 
    batch_size=batch_size, 
    shuffle=True,
     num_workers=workers,
    # num_workers=os.cpu_count(),
    pin_memory=True, 
    drop_last=True)
```

```python
# k
	#re_dataset
flowers_ds = CustomDataset(
    list_images=glob.glob("/kaggle/input/flowers-recognition/flowers/flowers/*/*.jpg"),
    transform=custom_transform
)

	#å±•ç¤ºè‡ªå®šä¹‰æ•°æ®é›†
plt.figure(figsize=(10,20))
def view_data(flowers, index):
    for i in range(1,6):
        images = flowers[index]
        view1, view2 = images
        plt.subplot(5,2,2*i-1)
        plt.imshow(view1.permute(1,2,0))
        plt.subplot(5,2,2*i)
        plt.imshow(view2.permute(1,2,0))

view_data(flowers_ds,2000)
	#re_dataloader
train_dl = torch.utils.data.DataLoader(
    flowers_ds,
    batch_size=BATCH_SIZE,
    shuffle=True,
    num_workers=os.cpu_count(),
    drop_last=True,
    pin_memory=True,
)
```

### 3 model

> resnet_simclr.py

```python
# s

self.criterion =torch.nn.CrossEntropyLoss().to(device)
features = self.model(images)
	#â­info_nce_loss
logits, labels = self.info_nce_loss(features)
loss = self.criterion(logits, labels)

	#arch:å¯é€‰resnet18/50
model = ResNetSimCLR(base_model=arch, out_dim=out_dim)
	#ğŸ‘‡ä¸ç”¨åŠ¨
optimizer = torch.optim.Adam(model.parameters(), lr, weight_decay=weight_decay)
	#ä½™å¼¦é€€ç«
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(train_loader), eta_min=0,last_epoch=-1)



    #  å¦‚æœgpuindexæ˜¯è´Ÿæ•°orä¸å­˜åœ¨å°±ä¼šno-opï¼ˆä»€ä¹ˆéƒ½ä¸åšï¼‰
    with torch.cuda.device(gpu_index):
        simclr = SimCLR(model=model, optimizer=optimizer, scheduler=scheduler)
        #ğŸ‘‡è§2
        simclr.train(train_loader)
```

```python
# k
	#ğŸ‘‡ä¸å˜
simclr_model = SimCLR().to(DEVICE)
criterion = nn.CrossEntropyLoss().to(DEVICE)
optimizer=torch.optim.Adam(simclr_model.parameters())


for i, views in enumerate(train_dl):#ğŸ‘ˆéå†dataloader
        features = simclr_model([view.to(DEVICE) for view in views])
        
        logits, labels = cont_loss(features, temp=2)
        loss = criterion(logits, labels)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        # print statistics
        running_loss += loss.item()
```

### 4 å¯¹æ¯”æŸå¤±

```python
#s
   def info_nce_loss2(self, features):
        
        #ã€1ã€‘

        labels = torch.cat([torch.arange(batch_size) for i in range(n_views)], dim=0)
        labels = (labels.unsqueeze(0) == labels.unsqueeze(1)).float()
        labels = labels.to(device)

        features = F.normalize(features, dim=1)

        similarity_matrix = torch.matmul(features, features.T)
        # assert similarity_matrix.shape == (
        #     n_views * batch_size, n_views * batch_size)
        # assert similarity_matrix.shape == labels.shape

        # discard the main diagonal from both: labels and similarities matrix
        mask = torch.eye(labels.shape[0], dtype=torch.bool).to(device)
        labels = labels[~mask].view(labels.shape[0], -1)
        similarity_matrix = similarity_matrix[~mask].view(similarity_matrix.shape[0], -1)
        # assert similarity_matrix.shape == labels.shape

        # slect and combine multiple positives
        positives = similarity_matrix[labels.bool()].view(labels.shape[0], -1)

        # select only the negatives the negatives
        negatives = similarity_matrix[~labels.bool()].view(similarity_matrix.shape[0], -1)

        logits = torch.cat([positives, negatives], dim=1)
        labels = torch.zeros(logits.shape[0], dtype=torch.long).to(device)

        logits = logits / temperature
        # ä½¿å¾—ç›¸ä¼¼åº¦æ›´åŠ å¹³æ»‘ï¼Œæ–¹ä¾¿è®­ç»ƒã€‚
        return logits, labels
```

```python
#k

 	#ã€1ã€‘
LABELS = torch.cat([torch.arange(BATCH_SIZE) for i in range(2)], dim=0)
LABELS = (LABELS.unsqueeze(0) == LABELS.unsqueeze(1)).float() # Creates a one-hot with broadcasting
LABELS = LABELS.to(DEVICE) #128,128

def cont_loss(features, temp):

    similarity_matrix = torch.matmul(features, features.T) # 128, 128
    # discard the main diagonal from both: labels and similarities matrix
    mask = torch.eye(LABELS.shape[0], dtype=torch.bool).to(DEVICE)
    # ~mask is the negative of the mask
    # the view is required to bring the matrix back to shape
    labels = LABELS[~mask].view(LABELS.shape[0], -1) # 128, 127
    similarity_matrix = similarity_matrix[~mask].view(similarity_matrix.shape[0], -1) # 128, 127

    # select and combine multiple positives
    positives = similarity_matrix[labels.bool()].view(labels.shape[0], -1) # 128, 1

    # select only the negatives
    negatives = similarity_matrix[~labels.bool()].view(similarity_matrix.shape[0], -1) # 128, 126

    logits = torch.cat([positives, negatives], dim=1) # 128, 127
    labels = torch.zeros(logits.shape[0], dtype=torch.long).to(DEVICE)

    logits = logits / temp
    return logits, labels
```

